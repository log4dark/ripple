# preprocess/audio/vad.py

import webrtcvad
import numpy as np
import wave
import os

# ğŸ“¥ WAV ì½ê¸°
def read_wave(path):
    with wave.open(path, 'rb') as wf:
        assert wf.getsampwidth() == 2  # 16-bit
        assert wf.getnchannels() == 1  # mono
        assert wf.getframerate() in (8000, 16000, 32000, 48000)
        pcm_data = wf.readframes(wf.getnframes())
        return pcm_data, wf.getframerate()

# ğŸ’¾ WAV ì €ì¥
def save_wave(path, audio_bytes, sample_rate):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with wave.open(path, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_bytes)

# ğŸ“¦ ì—…ë¡œë“œìš© VAD ì „ì²˜ë¦¬ â†’ ìŒì„± ìˆëŠ” êµ¬ê°„ë§Œ í•„í„°ë§
def split_voiced(path, aggressiveness=2, frame_duration_ms=30):
    audio, sample_rate = read_wave(path)
    vad = webrtcvad.Vad(aggressiveness) # 0(ë¯¼ê°) ~ 3(ëœ ë¯¼ê°)
    frame_size = int(sample_rate * frame_duration_ms / 1000) * 2  # byte ë‹¨ìœ„
    segments = []

    for i in range(0, len(audio), frame_size):
        frame = audio[i:i + frame_size]
        if len(frame) < frame_size:
            break
        if vad.is_speech(frame, sample_rate):
            segments.append(frame)

    voiced_audio = b''.join(segments)
    return voiced_audio, sample_rate

# ğŸ”„ ì‹¤ì‹œê°„ í”„ë ˆì„ìš© VAD â†’ bool ë°˜í™˜
def is_speech_frame(frame_bytes, sample_rate, aggressiveness=2):
    vad = webrtcvad.Vad(aggressiveness)
    return vad.is_speech(frame_bytes, sample_rate)

# ğŸ›  ì‹¤ì‹œê°„ í”„ë ˆì„ ìƒì„± ìœ í‹¸
def frame_generator(pcm_array, sample_rate, frame_duration_ms=30):
    frame_size = int(sample_rate * frame_duration_ms / 1000)
    for i in range(0, len(pcm_array), frame_size):
        frame = pcm_array[i:i + frame_size]
        if len(frame) < frame_size:
            break
        yield frame

