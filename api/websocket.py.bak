# api/websocket.py

from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import os
from datetime import datetime
import wave
import numpy as np

from utils.log import setup_logger
from model.loader import get_model
from model.transcribe import stream_transcribe

router = APIRouter()
logger = setup_logger("websocket")

AUDIO_DIR = "temp_audio"
os.makedirs(AUDIO_DIR, exist_ok=True)

SAMPLE_RATE = 16000
CHUNK_SECONDS = 5
OVERLAP_SECONDS = 0.5
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_SECONDS)
OVERLAP_SIZE = int(SAMPLE_RATE * OVERLAP_SECONDS)

MUTE_TRIGGER_DBFS = -45.0
MUTE_TRIGGER_DURATION = 1.0  # ì´ˆ ë‹¨ìœ„
MUTE_TRIGGER_MINLEN = int(SAMPLE_RATE * MUTE_TRIGGER_DURATION)

model = get_model()

@router.websocket("/ws/stt/stream")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
    filepath = os.path.join(AUDIO_DIR, filename)

    frames = bytearray()
    buffer = []
    prev_tail = np.array([], dtype=np.int16)
    silence_buffer = np.array([], dtype=np.float32)

    logger.info(f"ğŸ™ï¸ WebSocket ì—°ê²° ìˆ˜ë¦½ë¨ â†’ {filename}")

    try:
        while True:
            data = await websocket.receive_bytes()
            frames.extend(data)

            chunk = np.frombuffer(data, dtype=np.int16)
            buffer.append(chunk)
            silence_buffer = np.concatenate([silence_buffer, chunk.astype(np.float32) / 32768.0])

            total = sum(len(b) for b in buffer)
            if total >= CHUNK_SIZE or len(silence_buffer) >= MUTE_TRIGGER_MINLEN:
                rms = np.sqrt(np.mean(silence_buffer ** 2))
                dbfs = 20 * np.log10(rms + 1e-10)

                if total >= CHUNK_SIZE or dbfs < MUTE_TRIGGER_DBFS:
                    logger.debug("[ì²­í¬ ì²˜ë¦¬ ì‹œì‘] ----------------------------")

                    current = np.concatenate(buffer)[:CHUNK_SIZE]
                    buffer = [np.concatenate(buffer)[CHUNK_SIZE:]]

                    if prev_tail.size > 0:
                        full = np.concatenate([prev_tail, current])
                    else:
                        full = current

                    prev_tail = current[-OVERLAP_SIZE:]

                    logger.debug(f"STT í”„ë ˆì„ ê¸¸ì´: {len(full)}")

                    segments, result = stream_transcribe(model, full, sr=SAMPLE_RATE)

                    filtered_segments = []
                    last_end = 0.0
                    TOL = 0.3

                    for seg in segments:
                        if seg.start >= last_end - TOL:
                            filtered_segments.append(seg)
                            last_end = seg.end
                        else:
                            logger.debug(f"ì¤‘ë³µ ì œê±°ë¨: {seg.start:.2f}-{seg.end:.2f} â†’ {seg.text}")

                    result_text = " ".join([s.text for s in filtered_segments]).strip()

                    if result_text:
                        logger.info(f"ì‹¤ì‹œê°„ STT ê²°ê³¼ (ì¤‘ë³µì œê±°): {result_text[:50]}...")
                    else:
                        logger.debug("ì‹¤ì‹œê°„ STT ê²°ê³¼ ì—†ìŒ")
                    logger.debug("[ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ] ----------------------------\n")

                    # âœ… ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì „ì†¡
                    # await websocket.send_text(result_text or "[ë¬´ìŒ ë˜ëŠ” ì¸ì‹ ì•ˆë¨]")
                    if result_text:
                        await websocket.send_text(result_text)

                    silence_buffer = np.array([], dtype=np.float32)

    except WebSocketDisconnect:
        logger.warning(f"â— WebSocket ì—°ê²° ëŠê¹€ â†’ {filename}")
        try:
            with wave.open(filepath, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(SAMPLE_RATE)
                wf.writeframes(frames)
            logger.info(f"ğŸ’¾ WAV ì €ì¥ ì™„ë£Œ â†’ {filepath} ({len(frames)} bytes)")
        except Exception as e:
            logger.error(f"ğŸš¨ WAV ì €ì¥ ì‹¤íŒ¨ â†’ {filepath} | ì˜¤ë¥˜: {str(e)}")

