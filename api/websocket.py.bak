# api/websocket.py

from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import os
from datetime import datetime
import wave
import numpy as np

from utils.log import setup_logger
from model.loader import get_model
from model.transcribe import stream_transcribe

router = APIRouter()
logger = setup_logger("websocket")

AUDIO_DIR = "temp_audio"
os.makedirs(AUDIO_DIR, exist_ok=True)

SAMPLE_RATE = 16000
CHUNK_SECONDS = 5
OVERLAP_SECONDS = 0.5
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_SECONDS)
OVERLAP_SIZE = int(SAMPLE_RATE * OVERLAP_SECONDS)

MUTE_TRIGGER_DBFS = -45.0
MUTE_TRIGGER_DURATION = 1.0  # 초 단위
MUTE_TRIGGER_MINLEN = int(SAMPLE_RATE * MUTE_TRIGGER_DURATION)

model = get_model()

@router.websocket("/ws/stt/stream")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
    filepath = os.path.join(AUDIO_DIR, filename)

    frames = bytearray()
    buffer = []
    prev_tail = np.array([], dtype=np.int16)
    silence_buffer = np.array([], dtype=np.float32)

    logger.info(f"🎙️ WebSocket 연결 수립됨 → {filename}")

    try:
        while True:
            data = await websocket.receive_bytes()
            frames.extend(data)

            chunk = np.frombuffer(data, dtype=np.int16)
            buffer.append(chunk)
            silence_buffer = np.concatenate([silence_buffer, chunk.astype(np.float32) / 32768.0])

            total = sum(len(b) for b in buffer)
            if total >= CHUNK_SIZE or len(silence_buffer) >= MUTE_TRIGGER_MINLEN:
                rms = np.sqrt(np.mean(silence_buffer ** 2))
                dbfs = 20 * np.log10(rms + 1e-10)

                if total >= CHUNK_SIZE or dbfs < MUTE_TRIGGER_DBFS:
                    logger.debug("[청크 처리 시작] ----------------------------")

                    current = np.concatenate(buffer)[:CHUNK_SIZE]
                    buffer = [np.concatenate(buffer)[CHUNK_SIZE:]]

                    if prev_tail.size > 0:
                        full = np.concatenate([prev_tail, current])
                    else:
                        full = current

                    prev_tail = current[-OVERLAP_SIZE:]

                    logger.debug(f"STT 프레임 길이: {len(full)}")

                    segments, result = stream_transcribe(model, full, sr=SAMPLE_RATE)

                    filtered_segments = []
                    last_end = 0.0
                    TOL = 0.3

                    for seg in segments:
                        if seg.start >= last_end - TOL:
                            filtered_segments.append(seg)
                            last_end = seg.end
                        else:
                            logger.debug(f"중복 제거됨: {seg.start:.2f}-{seg.end:.2f} → {seg.text}")

                    result_text = " ".join([s.text for s in filtered_segments]).strip()

                    if result_text:
                        logger.info(f"실시간 STT 결과 (중복제거): {result_text[:50]}...")
                    else:
                        logger.debug("실시간 STT 결과 없음")
                    logger.debug("[청크 처리 완료] ----------------------------\n")

                    # ✅ 결과가 있을 때만 전송
                    # await websocket.send_text(result_text or "[무음 또는 인식 안됨]")
                    if result_text:
                        await websocket.send_text(result_text)

                    silence_buffer = np.array([], dtype=np.float32)

    except WebSocketDisconnect:
        logger.warning(f"❗ WebSocket 연결 끊김 → {filename}")
        try:
            with wave.open(filepath, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(SAMPLE_RATE)
                wf.writeframes(frames)
            logger.info(f"💾 WAV 저장 완료 → {filepath} ({len(frames)} bytes)")
        except Exception as e:
            logger.error(f"🚨 WAV 저장 실패 → {filepath} | 오류: {str(e)}")

